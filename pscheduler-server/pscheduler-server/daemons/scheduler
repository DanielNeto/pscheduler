#!/usr/bin/python
#
# pScheduler Run Scheduler
#

import daemon
import datetime
import errno
import optparse
import pscheduler
import psycopg2
import psycopg2.extensions
import random
import re
import requests
import select
import socket
import sys
import time
import traceback
import urlparse

# Gargle the arguments

opt_parser = optparse.OptionParser()

# Daemon-related options

opt_parser.add_option("--daemon",
                      help="Daemonize",
                      action="store_true",
                      dest="daemon", default=False)
opt_parser.add_option("--pid-file",
                      help="Location of PID file",
                      action="store", type="string", dest="pidfile",
                      default=None)

# Program options

# TODO: Do we want pscheduler as the default here?
opt_parser.add_option("-d", "--dsn",
                      help="Database connection string",
                      action="store", type="string", dest="dsn",
                      default="dbname=pscheduler")
opt_parser.add_option("-r", "--refresh",
                      help="Forced refresh interval (ISO8601)",
                      action="store", type="string", dest="refresh",
                      default="PT10S")
opt_parser.add_option("-v", "--verbose", action="store_true", dest="verbose")
opt_parser.add_option("--debug", action="store_true", dest="debug")

(options, args) = opt_parser.parse_args()

refresh = pscheduler.iso8601_as_timedelta(options.refresh)
if refresh is None:
    opt_parser.error('Invalid refresh interval "' + options.refresh + '"')
if pscheduler.timedelta_as_seconds(refresh) == 0:
    opt_parser.error("Refresh interval must be calculable as seconds.")

log = pscheduler.Log(verbose=options.verbose, debug=options.debug)

dsn = options.dsn

# Minimum amount of time from now when the first run of a task can be
# scheduled.  This prevents "start now" tasks from being scheduled for
# a time before the participants can prepare for them.
# TODO: Potential race condition?  Yep.
first_run_offset = pscheduler.iso8601_as_timedelta('PT10S')


#
# Run Poster
#


def run_post(
    task_url_text, # URL for task
    start_time,    # Desired start time
    log=None
    ):
    """
    Schedule a run of a task on all participating nodes.

    Returns a tuple containing a list of the posted run URLs (which
    will be None in the event of an error) and an error message (None
    if there was no error).
    """

    log and log.debug("Posting %s at %s", task_url_text, start_time)

    # TODO: Error handling when calling url_* needs improvement.  Lots.

    task_url = urlparse.urlparse(task_url_text)
    assert type(start_time) == datetime.datetime

    status, task = pscheduler.url_get(task_url_text, params={'detail': 1})


    # Generate a list of the task URLs

    task_urls =[]
    participants = task['detail']['participants']
    log and log.debug("Participant list is %s", participants)
    assert len(participants) >= 1

    parts = list(task_url)
    for participant in participants:
        # TODO: Use the canonicalizer for generating these URLs.
        parts[1] = re.sub( '^[^:]*',
                           pscheduler.api_this_host()
                           if participant is None else str(participant),
                           parts[1])
        url = urlparse.urlunsplit(parts[:-1])
        task_urls.append(url)
        log and log.debug("Participant task URL is %s", url)

    #
    # Figure out the range of times in which the task can be run.
    #

    task_duration = pscheduler.iso8601_as_timedelta(task['detail']['duration'])
    try:
        task_slip = pscheduler.iso8601_as_timedelta(task['detail']['slip'])
    except KeyError:
        task_slip = datetime.timedelta()

    run_range_end = start_time + task_duration + task_slip

    range_params = {
        'start': pscheduler.datetime_as_iso8601(start_time),
        'end': pscheduler.datetime_as_iso8601(run_range_end)
        }
    log and log.debug("Range parameters are %s", str(range_params))


    #
    # Get a list of the time ranges each participant has available to
    # run the task that overlap with the range we want.
    #

    range_set = []

    log and log.debug("%s URLs in this task", len(task_urls))

    for task_url in task_urls:

        # TODO: It would be nice if the task had a list of the
        # runtimes URLs so we don't have to build it.

        # TODO: Remove this
        if True:
            log.debug("TEST Fetching task")
            stat, result = pscheduler.url_get(task_url, throw=False)
            log.debug("Status = %s", stat)


        runtime_url = task_url + '/runtimes'

        log.debug("Fetching %s", runtime_url)


        # TODO: These two blocks of code are supposed to be
        # equivalent, but the call to url_get() causes this call to
        # just return for no apparent reason.  The version that uses
        # requests is added below as a temporary fix until fixed.  See
        # issue #116

        #status, json_ranges = pscheduler.url_get(runtime_url,
        #                                         params=range_params,
        #                                         Throw=False)

        r = requests.get(runtime_url, params=range_params, verify=False)
        status = r.status_code

        # TODO: In either of these cases, should probably delete any
        # runs that were scheduled before returning.

        if status != 200:
            return (None, None, None,
                    "Error trying to schedule with %s: %s %d"
                    % (participant, runtime_url, status))

        json_ranges = pscheduler.json_load(r.text)

        if len(json_ranges) == 0:
            return (None, None, None,
                    "%s cannot schedule a time for this run" % participant)

        log and log.debug("Scheduled against %s", task_url)
        
        range_set.append( [ (pscheduler.iso8601_as_datetime(item['lower']),
                             pscheduler.iso8601_as_datetime(item['upper']))
                            for item in json_ranges ] )


    log and log.debug("Done fetching time ranges")

    #
    # Find the range that fits
    #

    # The adjustment of the duration by one second forces
    # coalesce_ranges() to behave like the ranges are closed instead
    # of half-closed.

    schedule_range = \
        pscheduler.coalesce_ranges( range_set,
                                    task_duration \
                                        - datetime.timedelta(seconds=1) )
    if schedule_range is None:
        return (None, None, None, "No mutually-agreeable time to run this task."
                + str(range_set) + ' ' + str(task_duration))

    (schedule_lower, schedule_upper) = schedule_range
    assert schedule_lower < schedule_upper
    log and log.debug("Time range is %s - %s", schedule_lower, schedule_upper)

    # Apply random slip if one was specified

    try:
        randslip = task['schedule']['randslip']
        slip_available = schedule_upper - schedule_lower - task_duration
        slip_seconds = pscheduler.timedelta_as_seconds(slip_available) \
            * random.random()
        schedule_lower += pscheduler.seconds_as_timedelta(int(slip_seconds))
        log and log.debug("Applying random slip of %d seconds",
                          int(slip_seconds))
    except KeyError:
        pass  # No random slip, no problem.

    # Make sure we haven't slipped further than allowed.
    assert schedule_upper - schedule_lower >= task_duration

    schedule_upper = schedule_lower + task_duration

    #
    # Post the runs to the participants
    #

    run_params = { 'start-time': schedule_lower.isoformat() }

    runs_posted = []

    # First one is the lead.  Post it and get the UUID.

    if log:
        log.debug("Posting lead run to %s", task_urls[0])
        log.debug("Data %s", run_params)
    status, run_lead_url \
        = pscheduler.url_post(task_urls[0] + '/runs',
                              data=pscheduler.json_dump(run_params),
                              throw=False,
                              json=True)
    log and log.debug("Lead URL is %s", run_lead_url)
    assert type(run_lead_url) in [str, unicode]
    runs_posted.append(run_lead_url)

    # TODO: This should parse the URL and change the netloc instead of
    # assembling URLs.

    # What to add to a task URL to make the run URL
    run_suffix = run_lead_url[len(task_urls[0]):]

    # Cover the rest of the participants if there are any.

    errors = []

    run_data = pscheduler.json_dump(run_params)

    for task_url in task_urls[1:]:

        put_url = task_url + run_suffix

        if log:
            log.debug("Putting run to participant %s", put_url)
            log.debug("Parameters: %s", run_params)

        status, output = pscheduler.url_put(put_url,
                                            data=run_data,
                                            throw=False,
                                            json=False  # No output.
                                            )

        log and log.debug("PUT %d: %s", status, output)

        if status != 200:
            log and log.debug("Failed: %s", output)
            errors.append(output)
            continue

        runs_posted.append(put_url)
        log and log.debug("Succeeded.")

    if len(runs_posted) != len(task_urls):
        log and log.debug("Removing runs: %s", runs_posted)
        pscheduler.url_delete_list(runs_posted)
        return (None, None, None,
                "Failed to post/put runs to all participants: %s"
                %  ("; ".join(errors))
        )

    #
    # Fetch all per-participant data, merge it and distribute the
    # result to all participants.
    #

    log and log.debug("Fetching per-participant data")

    part_data = []

    for run in runs_posted:

        # TODO: Should this be multiple attempts to avoid a race condition?
        log and log.debug("Getting part data from %s", run)
        status, result = pscheduler.url_get(run, throw=False)
        if status != 200 or not 'participant-data' in result:
            log.debug("Deleting runs: %s", runs_posted)
            pscheduler.url_delete_list(runs_posted)
            # TODO: Better error?
            return (None, None, None, "Failed to get run data from all participants")
        part_data.append(result['participant-data'])
        log and log.debug("Got %s", result['participant-data'])

    full_data = pscheduler.json_dump ({
        'part-data-full': part_data
        })

    log and log.debug("Full part data: %s", full_data)

    for run in runs_posted:
        log and log.debug("Putting full part data to %s", run)
        status, result = pscheduler.url_put(run,
                                            data=full_data,
                                            json=False,
                                            throw=False)
        if status != 200:
            pscheduler.url_delete_list(runs_posted)
            # TODO: Better error?
            log and log.debug("Failed: %s", result)
            return (None, None, None, "Failed to post run data to all participants")


    # TODO: Probably also want to return the start and end times?
    log and log.debug("Run posting finished")
    return (runs_posted[0], schedule_lower, schedule_upper, None)


#
# Main Program
#

def main_program():

    # TODO: All DB transactions need to be error checked

    pg = pscheduler.pg_connection(dsn)
    cursor = pg.cursor()
    cursor.execute("LISTEN task_change")

    # This is separate because inserts of non-starters happen while
    # fetching rows from the other cursor.
    nonstart_cursor = pg.cursor()


    while True:

        wait = True

        cursor.execute("""
            SELECT uuid, runs, trynext, FALSE
            FROM schedule_runs_to_schedule
            """)

        # Check if any notifications arrived while this query executed.
        if pg.notifies:
            wait = False
            del pg.notifies[:]
            log.debug("Received notifications.")

        # Any rows returned means we query again.
        if cursor.rowcount > 0:
            log.debug("Got %d rows", cursor.rowcount)
            wait = False


        for row in cursor:

            uuid, runs, trynext, background = row

            log.debug("%sTASK %s, %d runs, try %s", 
                      "BACKGROUND " if background else "",
                      uuid, runs, trynext)


            url = pscheduler.api_url(path='/tasks/' + uuid)

            # For the first run only, push the start time out.
            # See comment above near the declaration of
            # first_run_offset.

            if runs == 0:
                later_start = pscheduler.time_now() + first_run_offset
                if trynext < later_start:
                    trynext = later_start                   

            log.debug("Trying to schedule %s for %s at %s",
                      uuid, trynext, url)
            log.debug("URL is %s", url)
            run_uri, start_time, end_time, error = \
                run_post(url, trynext, log)

            if error is not None:
                log.debug("Unable: %s", error)
                # Post a non-starting run on the lead (local) system
                try:
                    nonstart_cursor.execute(
                        """SELECT api_run_post(%s, normalized_now(), NULL, %s)""",
                        [uuid, error])
                except:
                    log.error("Insertion of non-starter failed: %s", error)

            else:
                log.debug("Scheduled for %s - %s at %s",
                          start_time, end_time, run_uri)

        # Wait for something to happen.
          
        if wait:

            try:
                if select.select([pg],[],[],
                                 pscheduler.timedelta_as_seconds(refresh)) \
                                 != ([],[],[]):
                    # Notified
                    pg.poll()
                    log.debug("Notified: %s", pg.notifies)
                    del pg.notifies[:]

            except select.error as ex:
                err_no, message = ex
                if err_no != errno.EINTR:
                    log.exception()
                    raise ex
                else:
                    log.debug("Interrupted")



if options.daemon:
    pidfile = pscheduler.PidFile(options.pidfile)
    with daemon.DaemonContext(pidfile=pidfile):
        pscheduler.safe_run(lambda: main_program())
else:
    pscheduler.safe_run(lambda: main_program())

