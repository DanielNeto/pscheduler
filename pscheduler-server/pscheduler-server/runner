#!/usr/bin/python
#
# Execute runs of tasks and put the results into the database.
#

# TODO: See which of these are still needed.
import detach
import errno
import optparse
import pscheduler
import psycopg2
import psycopg2.extensions
import select
import sys
import time


# Gargle the arguments

opt_parser = optparse.OptionParser()

opt_parser.add_option("-c", "--channel",
                      help="Schedule notification channel",
                      action="store", type="string", dest="channel",
                      default="run_change")

opt_parser.add_option("-d", "--dsn",
                      help="Database connection string",
                      action="store", type="string", dest="dsn",
                      default="")

# NOTE: Changing the refresh interval will have an effect on how many
# forked processes are running, connected to the database and waiting
# for a run to start.  Increase this value only if you understand the
# consequences.
opt_parser.add_option("-r", "--refresh",
                      help="Forced refresh interval (ISO8601)",
                      action="store", type="string", dest="refresh",
                      default="PT1M")

opt_parser.add_option("--verbose", action="store_true", dest="verbose")
opt_parser.add_option("--debug", action="store_true", dest="debug")

(options, args) = opt_parser.parse_args()

refresh = pscheduler.iso8601_as_timedelta(options.refresh)
if refresh is None:
    opt_parser.error('Invalid refresh interval "' + options.refresh + '"')
if pscheduler.timedelta_as_seconds(refresh) == 0:
    opt_parser.error("Refresh interval must be calculable as seconds.")


log = pscheduler.Log(verbose=options.verbose, debug=options.debug)

dsn = options.dsn



#
# Runs and things we do with them
#
class Run:

    def __init__(self, id, start_in):

        try:

            # TODO: May not want to fully detach so children die with
            # the parent.

            with detach.Detach(close_fds=False,
                               stdout=sys.stdout,
                               stderr=sys.stderr) as detacher:

                if detacher.pid:
                    self.pid = detacher.pid
                    return

                log.debug("Started runner for ID %d", id)

                # TODO: Need to find a nice way to catch and log
                # exceptions from these.  safe_run() doesn't quite cut
                # it.

                self._run(id, start_in)

        except:
            log.exception()


    # This method does the dirty work.  Don't worry about catching
    # exception; that's the caller's responsibility.
    def _run(self, id, start_in):

        db = pscheduler.pg_connection(dsn)
        cursor = db.cursor()

        # Don't try to do anyting until the start time.
        sleep_time = pscheduler.timedelta_as_seconds(start_in)

        log.debug("%d: Sleeping %s until test start", id, sleep_time)
        time.sleep(sleep_time)

        cursor.execute("""
                       UPDATE run
                       SET state = run_state_running()
                       WHERE id = %s
                       """, [id])

        cursor.execute("""
                       SELECT
                           tool.name,
                           task.uuid,
                           task.id,
                           task.participant,
                           task.participants,
                           lower(run.times),
                           upper(run.times),
                           task.json #> '{test}',
                           run.uuid,
                           run.part_data_full
                       FROM
                           run
                           JOIN task ON task.id = run.task
                           JOIN tool ON tool.id = task.tool
                       WHERE run.id = %s
                       """, [id])

        # TODO: Should get exactly one row back.
        row = cursor.fetchone()

        tool, task_uuid, task_id, participant, participants, start, end, \
            test_spec, run_uuid, partdata = row

        #
        # Do the local tool run
        #

        if partdata is None:
            log.error("%d: Got NULL part_data_full", id)

        tool_input = {
            'schema': 1,
            'schedule': {
                'start': pscheduler.datetime_as_iso8601(start),
                'duration': pscheduler.timedelta_as_iso8601(end - start)
                },
            'test': test_spec,
            'participant': participant,
            'participant-data': partdata
            }

        tool_input = pscheduler.json_dump(tool_input)
        log.debug("%d: Testing with %s: %s", id, tool, tool_input)

        returncode, stdout, stderr = pscheduler.run_program(
            [ "pscheduler", "internal", "invoke", "tool", tool, "run" ],
            stdin = tool_input,
            # TODO: Make the overage on the timeout configurable, or
            # is a couple of seconds beyond what the tool said it
            # would take sufficient?
            timeout = pscheduler.timedelta_as_seconds(end - start) + 2
            )

        if len(stdout) == 0:
            stdout = None

        if len(stderr) == 0:
            stderr = None

        if returncode == 0:
            log.debug("%d: Test Succeeded: %s", id, stdout)
        else:
            log.debug("%d: Test failed %d: %s", id, returncode, stderr)


        # TODO: Put the task into the cleanup state and take the
        # automatic trigger out of the database.  Should go
        # direct to finished if not the lead participant.

        # TODO: Error check this.
        cursor.execute("""
                       UPDATE run
                       SET
                           status = %s,
                           result = %s,
                           errors = %s
                       WHERE id = %s
                       """,
                       [returncode,
                        stdout,
                        stderr,
                        id])

        # Note that the run happened.
        # TODO: Error check this.
        cursor.execute("SELECT task_register_run(%s)", [task_id])

        log.debug("%d: Stored local result", id)

        # The lead participant takes care of gathering and merging the finished results.

        if participant == 0:

            log.debug("%d: Doing lead participant duties", id)
           
            # Wait until the scheduled time has passed, which is the
            # only time we can be sure results might be available.

            # TODO:  Skip sleep if this is a single-participant test?

            wait_time = pscheduler.time_until_seconds(end)
            log.debug("%d: Waiting for task end time to pass (%s)",
                      id, wait_time)
            time.sleep(wait_time)
            log.debug("%d: Task end time has passed", id)

            # Fetch and combine the results.

            runs = [ pscheduler.api_url(host = host,
                                        path = '/tasks/%s/runs/%s'
                                        % (task_uuid, run_uuid) )
                     for host in participants ]

            log.debug("%d: Runs are %s", id, runs)


            # TODO: Shouldn't be doing this if the local run failed.

            try:
                log.debug("Local run returned %d", returncode)
                if returncode == 0:
                    local_result = pscheduler.json_load(stdout)
                else:
                    local_result = None
                    log.debug("%d: Tool returned failure: %s", id, stderr)
            except ValueError as ex:
                log.error("%d: Tool %s returned invalid JSON %s",
                          id, tool, stdout)
                local_result = None


            full_result = [ local_result ]
            log.debug("%d: Accumulated local result", id)

            for run in runs[1:]:

                log.debug("%d: Fetching run %s", id, run)

                status, result = pscheduler.url_get( run,
                                     params={ 'wait-local': True },
                                     throw=False )

                if status == 200:
                    log.debug("%d: Retrieved %s", id, result)
                    full_result.append(result['result'])
                else:
                    log.warning("%d: Unable to retrieve run %s", id, run)
                    full_result.append(None)


            log.debug("%d: Full result: %s",
                      id, pscheduler.json_dump(full_result, pretty=True))

            # TODO: If any of the tests resulted in a failure, check
            # if the participants' clocks are off and add that to the
            # diagnostics.

            # Store the full result with each participant.

            full_params = {
                'run': pscheduler.json_dump({ 'result-full' : full_result })
                }


            # TODO: Store the local result in the database instead of
            # doing it through the REST API.

            for run in runs:
                log.debug("%d: Storing result in %s", id, run)
                status, returned = pscheduler.url_put(run,
                                                      params=full_params,
                                                      throw=False,
                                                      json=False)
                if status != 200:
                    log.warning("%d: Unable to update run %s: %d %s",
                                id, run, status, returned)


        # TODO: Put the task into the finished state

        cursor.close()
        db.close()
        log.debug("%d: Run complete", id)




#
# Main Program
#

def main_program():

    pg = pscheduler.pg_connection(dsn)
    cursor = pg.cursor()
    cursor.execute("LISTEN " + options.channel)


    while True:

        # Operate only on runs that are scheduled to start before the next
        # forced refresh.

        cursor.execute("""SELECT
                              run,
                              start_in
                          FROM
                              schedule_upcoming
                          WHERE
                              start_in < %s
                          ORDER BY start_in
                   """, [refresh]);

        if cursor.rowcount:

            # TODO: There must be a nicer way to take this array slice as args.
            rows = cursor.fetchall()
            log.debug(str([ "RUN " + str(row[0]) + " " + str(row[1]) for row in rows ]) )
            runs = [ Run(row[0], row[1]) for row in rows ]
            wait_time = rows[0][1]

            # Do this here to guarantee that we don't pick up rows for
            # runs we just started in the next iteration of the loop.

            cursor.execute( "UPDATE run SET state = run_state_on_deck() WHERE id in %s",
                            (tuple([row[0] for row in rows]),) )
            log.debug("Put %d runs on deck", len(runs))

        else:

            log.debug("Nothing to do.")
            runs = []
            wait_time = refresh


        if not pscheduler.timedelta_is_zero(wait_time):

            # Wait for a notification or the wait time to elapse.  Eat all
            # notifications as a group; we only care that we were notified.

            log.debug("Next run or check in %s", wait_time)

            # TODO: This try needs to be brought to the other programs.
            # Better, make it a function in db.py.

            try:
                if select.select([pg],[],[],
                                 pscheduler.timedelta_as_seconds(wait_time)) \
                                 != ([],[],[]):
                    # Notified
                    pg.poll()
                    del pg.notifies[:]
                    log.debug("Schedule change.")

            except select.error as ex:

                err_no, message = ex
                if err_no != errno.EINTR:
                    log.exception()
                    raise ex

    # Not that this will ever be reached...
    pg.close()



pscheduler.safe_run(lambda: main_program())
