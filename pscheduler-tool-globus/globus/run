#!/usr/bin/python

#
# Development Order #5:
#
# This is the meat and bones of the tool, where the actual desired
# commands or operation will be run. The results are then recorded
# and added to the 'results' JSON data, which will then be sent
# back to the test. Both system and api are able to be used here.
#

import os
import sys
import json
import time
import datetime
import subprocess

import pscheduler

# from stdin
input = pscheduler.json_load(exit_on_error=True)

# Take input from test spec
try:
    dest = input['test']['spec']['dest']
    source = input['test']['spec']['source']
    parallel = input['test']['spec']['parallel']

except KeyError as e:
    print(e);	pscheduler.fail('Missing data in input')

if 'cleanup' in input['test']['spec']:
  cleanup = input['test']['spec']['cleanup']
else:
  cleanup = False


duration = input['test']['spec'].get('duration', 'PT5S')
duration = pscheduler.timedelta_as_seconds( pscheduler.iso8601_as_timedelta(duration) ) 
timeout_iso = input['test']['spec'].get('timeout', 'PT10S')
timeout = pscheduler.timedelta_as_seconds( pscheduler.iso8601_as_timedelta(timeout_iso) )
start_time = datetime.datetime.now()
succeeded = False
error = ''
diags = ''

# Run the actual task here:
bits_sent = 0.0
thru_put = 0.0

if True:

  argv = ['globus-url-copy', # Implimentation of GridFTP
          '-vb'] # verbose output for test

  ## Options handling ##

  # Number of parallel streams for GridFTP
  if parallel > 1:
    argv.append("-p")
    argv.append(str(parallel))
 
  #if options != " ":
  #  options = options.split(" ")
  #  for opt in options:
  #    argv.append(opt)


  argv.append(source) # Source URL
  argv.append(dest) # Destination URL
  
  status, stdout, stderr = pscheduler.run_program(argv, timeout=timeout)
 
  #with open("/tmp/debug.out", "wb") as f: 
    #[ f.write(s + "\n") for s in stdout]  
  #  f.write(stdout)                      


  # Parsing stdout for throughput and bytes-sent 
  orig_stdout = stdout
  if len(stdout) > 3: 
    stdout = stdout.split("\n")[4]
    stdout = stdout.split(" ")

  if len(stdout) > 2: 
    bits_sent = float(stdout[12])
    thru_put = float(stdout[22])
  
  if status:
    succeeded = False
    error = "Error running program:\n%s"% stderr.strip('\n')

  else:
    succeeded = True
    diags = orig_stdout

end_time = datetime.datetime.now()

if cleanup is True:
    if os.path.isfile(dest): 
        os.remove(dest)


#bits_sent = 0.0
#thru_put = 0.0
# Determine how much (if any) of the file was transferred
#if succeeded and os.path.isfile(dest):
#  bits_sent = os.stat(dest).st_size
  # Convert bytes to Megabytes
  #scalar = ['B', 'KB', 'MB']
  #while len(scalar) > 1:
  #  scalar = scalar[1:]
  #  bits_sent = bits_sent / 1024.0
#  thru_put = bits_sent / (end_time - start_time)

# Organize results into json data
final_results = {
    'succeeded': succeeded,
    'error': error,
    'diags': diags }

results = {"schema": 1}
results['time'] = pscheduler.timedelta_as_iso8601( end_time - start_time)
results['succeeded'] = succeeded
#if bits_sent != "": 
results["bytes-sent"] = bits_sent
#if thru_put != "":
results["throughput"] = thru_put

final_results['result'] = results
pscheduler.succeed_json(final_results)
#?
